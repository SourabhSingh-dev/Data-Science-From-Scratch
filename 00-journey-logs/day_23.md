# ğŸ—“ï¸ Day 22 â€“ 17/09/2025  

### ğŸ“ Status: âœ… Completed  

---

## âœ… What I did today:  

### ğŸ“Œ Topics Reviewed & Practiced:  
- ğŸ”» **Gradient Descent (Introduction & Intuition)**  
  - Understood the core idea of **iterative optimization**  
  - Learned update rule for parameters  
    - Example: `b_new = b_old - Î· * (loss_slope)`  

---

## ğŸ§© Practice & Execution:  
- Wrote a basic gradient descent loop to update parameter `b`  
- Observed how values converge step by step  
- Gained intuition on **learning rate (Î·)** and its impact on convergence  

---

## ğŸ“˜ Resources Used:  
- ğŸ“„ Notes on Gradient Descent basics  
- ğŸ”— YouTube videos  
- ğŸ§ª Jupyter Notebook experiments  

---

## ğŸ”„ Next Up:  
- Extend gradient descent to update **both m & b simultaneously**  
- Complete the mathematical derivation for parameter updates  
- Visualize convergence of cost function  

---

## ğŸ“ Reflections:  
- Gradient descent feels like a **trial-and-error learning process** but with math  
- Seeing values converge through loops gave solid intuition  
- Excited to move towards full **linear regression training with gradient descent**  
