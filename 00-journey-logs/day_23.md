# 🗓️ Day 22 – 17/09/2025  

### 📍 Status: ✅ Completed  

---

## ✅ What I did today:  

### 📌 Topics Reviewed & Practiced:  
- 🔻 **Gradient Descent (Introduction & Intuition)**  
  - Understood the core idea of **iterative optimization**  
  - Learned update rule for parameters  
    - Example: `b_new = b_old - η * (loss_slope)`  

---

## 🧩 Practice & Execution:  
- Wrote a basic gradient descent loop to update parameter `b`  
- Observed how values converge step by step  
- Gained intuition on **learning rate (η)** and its impact on convergence  

---

## 📘 Resources Used:  
- 📄 Notes on Gradient Descent basics  
- 🔗 YouTube videos  
- 🧪 Jupyter Notebook experiments  

---

## 🔄 Next Up:  
- Extend gradient descent to update **both m & b simultaneously**  
- Complete the mathematical derivation for parameter updates  
- Visualize convergence of cost function  

---

## 📝 Reflections:  
- Gradient descent feels like a **trial-and-error learning process** but with math  
- Seeing values converge through loops gave solid intuition  
- Excited to move towards full **linear regression training with gradient descent**  
